from dynaquery.config.settings import LLM_MODEL
from langchain_google_genai import ChatGoogleGenerativeAI
from pydantic import BaseModel, Field
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import PydanticOutputParser
from typing import Optional

# --- Define the Structured Output Schema ---
class ClassificationDecision(BaseModel):
    """A structured object containing the classification decision."""
    explanation: str = Field(description="A single-sentence explanation for the classification decision.")
    label: str = Field(description="The final classification label, must be one of ACCEPT, RECOMMEND, or REJECT.")

# --- LLM-Native Classification Pipeline ---
def get_llm_native_classifier_chain():
    """Initializes an LLM chain that produces structured, parsable output."""
    llm = ChatGoogleGenerativeAI(model=LLM_MODEL, temperature=0)
    parser = PydanticOutputParser(pydantic_object=ClassificationDecision)

    # --- PROMPT SELECTION ---
    # The active prompt is the Rule-Based prompt, our final recommended architecture.
    # To reproduce the results for the Descriptive Prompt (as described in the README),
    # comment out the "Rule-Based Prompt" below and uncomment the "Descriptive Prompt".

    # === ACTIVE: Rule-Based Prompt (Default) ===
    prompt_template_str = """
You are a meticulous classification expert. Your task is to analyze a user's question and a reasoning text to classify the outcome according to a strict rubric.
---
**CLASSIFICATION RUBRIC:**

- **ACCEPT** ‚Üí ALL conditions from the user's question are satisfied.  
- **RECOMMEND** ‚Üí SOME (at least one) conditions are satisfied, but not all. Treat this as a "partial match" similar to a recommender system.  
- **REJECT** ‚Üí NONE of the conditions are satisfied.  

‚ö†Ô∏è Important: If even ONE condition is satisfied, but others fail, you MUST classify as **RECOMMEND**, not REJECT. Only assign REJECT when ZERO conditions match.
---
**User's Question:** "{question}"
**Reasoning Text:**
{reasoning_text}
---
Based on your analysis, provide your response in the requested format.
{format_instructions}
    """

    # === INACTIVE: Descriptive Prompt (For Reproducibility) ===
    # To use this prompt, comment out the prompt_template_str above and uncomment the block below.
    # prompt_template_str = """
# You are a meticulous classification expert. Your task is to analyze a user's question and a reasoning text to classify the outcome according to a strict rubric.
# ---
# **CLASSIFICATION RUBRIC:**
# 
# *   **ACCEPT:** Choose this if the reasoning text, on its own, provides sufficient and complete evidence to unambiguously satisfy all logical requirements of the question.
#     *   For an "AND" question, the reasoning must explicitly or through direct semantic equivalence confirm that ALL conditions are met.
#     *   The reasoning must stand on its own as a complete, positive confirmation without reservation.
# 
# *   **RECOMMEND:** Choose this if the reasoning confirms the record's relevance but simultaneously indicates that the logical requirements are not perfectly met.
#     *   Partiality ‚Äì Some, but not all, of the required "AND" conditions are confirmed, or one is missing/uncertain.
#     *   Qualification ‚Äì A condition is confirmed but with a limitation, drawback, or flaw.
# 
# *   **REJECT:** Choose this if the reasoning confirms that none of the critical conditions are met or the record is clearly irrelevant.
# ---
# **User's Question:** "{question}"
# **Reasoning Text:**
# {reasoning_text}
# ---
# Based on your analysis, provide your response in the requested format.
# {format_instructions}
    # """
    
    prompt = PromptTemplate(
        template=prompt_template_str,
        input_variables=["question", "reasoning_text"],
        partial_variables={"format_instructions": parser.get_format_instructions()}
    )
    
    chain = prompt | llm | parser
    return chain

def classify_with_llm(user_query: str, llm_rationale: str, classifier_chain) -> Optional[ClassificationDecision]:
    """
    Classifies a rationale using the LLM-native chain.

    Args:
        user_query: The original user question.
        llm_rationale: The rationale generated by the upstream LLM.
        classifier_chain: The pre-initialized LangChain for classification.

    Returns:
        Optional[ClassificationDecision]: The parsed Pydantic object on success, None on failure.
    """
    try:
        # Debug line to see the prompt 
        full_prompt = classifier_chain.get_prompts()[0].format(
            question=user_query,
            reasoning_text=llm_rationale
        )
        print("üìù [DEBUG] Full prompt for Classification LLM:\n", full_prompt)
        
        response_obj = classifier_chain.invoke({
            "question": user_query,
            "reasoning_text": llm_rationale
        })
        return response_obj
            
    except Exception as e:
        print(f"WARNING: LLM-native classifier failed. Error: {e}")
        return None # Return None on failure